<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(r => {
              setTimeout(r);
            })(function renderToolbar() {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
            })(() => window.markmap,null,{"content":"&#x2696;&#xfe0f; Methods for Handling Imbalanced Datasets","children":[{"content":"1. Data-Level Methods","children":[{"content":"Random Oversampling","children":[{"content":"<pre data-lines=\"16,23\"><code>- Duplicate samples from minority class\n- &#x2705; Pros: Simple, fast, easy to implement\n- &#x274c; Cons: Overfitting risk (duplicate data)\n- &#x1f9e0; Best for: Tabular, image, simple text\n- &#x26a0;&#xfe0f; For time series: Use window-based variation\n- &#x1f3af; Suitable for: Classification (binary, multi-class)\n- &#x1f4c2; Data type: Tabular, image, text\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"16,23"}}],"payload":{"tag":"h3","lines":"14,15"}},{"content":"SMOTE (Synthetic Minority Over-sampling Technique)","children":[{"content":"<pre data-lines=\"26,33\"><code>- Generate synthetic samples via interpolation\n- &#x2705; Pros: Reduces overfitting vs pure duplication\n- &#x274c; Cons: Can generate noisy points, not ideal for high-dimensional or categorical data\n- &#x1f9e0; Best for: Tabular numerical data\n- &#x26a0;&#xfe0f; Not ideal for: Text, raw images, time series\n- &#x1f3af; Suitable for: Classification (binary, multi-class)\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"26,33"}}],"payload":{"tag":"h3","lines":"24,25"}},{"content":"ADASYN","children":[{"content":"<pre data-lines=\"36,42\"><code>- Adaptive SMOTE: more focus on difficult samples\n- &#x2705; Pros: Focuses on hard-to-learn minority points\n- &#x274c; Cons: Risk of overfitting to outliers\n- &#x1f9e0; Best for: Tabular with imbalanced boundary\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"36,42"}}],"payload":{"tag":"h3","lines":"34,35"}},{"content":"Borderline-SMOTE","children":[{"content":"<pre data-lines=\"45,51\"><code>- SMOTE near decision boundary\n- &#x2705; Pros: Improves decision region\n- &#x274c; Cons: Sensitive to noisy border\n- &#x1f9e0; Use when class boundary is fuzzy\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"45,51"}}],"payload":{"tag":"h3","lines":"43,44"}},{"content":"Random Undersampling","children":[{"content":"<pre data-lines=\"54,60\"><code>- Remove samples from majority class\n- &#x2705; Pros: Fast, reduces size\n- &#x274c; Cons: Risk of losing important info\n- &#x1f9e0; Best for: Large datasets with clear separation\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"54,60"}}],"payload":{"tag":"h3","lines":"52,53"}},{"content":"Tomek Links","children":[{"content":"<pre data-lines=\"63,69\"><code>- Remove overlapping majority samples\n- &#x2705; Pros: Cleaner boundary\n- &#x274c; Cons: Removes only specific cases\n- &#x1f9e0; Use for: Cleaning decision boundary\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"63,69"}}],"payload":{"tag":"h3","lines":"61,62"}},{"content":"NearMiss","children":[{"content":"<pre data-lines=\"72,78\"><code>- Keep majority samples closest to minority\n- &#x2705; Pros: Helps balance data geometrically\n- &#x274c; Cons: Computationally expensive\n- &#x1f9e0; Good for: Structured tabular datasets\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"72,78"}}],"payload":{"tag":"h3","lines":"70,71"}},{"content":"Edited Nearest Neighbours (ENN)","children":[{"content":"<pre data-lines=\"81,87\"><code>- Remove noisy samples based on k-NN rules\n- &#x2705; Pros: Cleans overlapping classes\n- &#x274c; Cons: Can remove useful samples\n- &#x1f9e0; Use with caution\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"81,87"}}],"payload":{"tag":"h3","lines":"79,80"}},{"content":"Hybrid Methods","children":[{"content":"<pre data-lines=\"90,96\"><code>- Combine over- and under-sampling\n- &#x2705; Pros: Combine benefits of both strategies\n- &#x274c; Cons: More complex, parameter tuning needed\n- &#x1f9e0; Use for: Mid-sized tabular datasets\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"90,96"}}],"payload":{"tag":"h3","lines":"88,89"}},{"content":"Data Augmentation","children":[{"content":"<pre data-lines=\"99,107\"><code>- Image: rotation, flip, zoom\n- Text: synonym replacement, back translation\n- Time series: jittering, time warping\n- &#x2705; Pros: Retains context, increases diversity\n- &#x274c; Cons: Domain-specific, needs manual design\n- &#x1f9e0; Best for: Image, text, time series\n- &#x1f3af; Suitable for: Classification, segmentation, object detection, generative\n- &#x1f4c2; Data type: Image, text, time series\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"99,107"}}],"payload":{"tag":"h3","lines":"97,98"}}],"payload":{"tag":"h2","lines":"12,13"}},{"content":"2. Algorithm-Level Methods","children":[{"content":"Class Weighting","children":[{"content":"<pre data-lines=\"112,118\"><code>- Adjust loss to penalize minority errors more\n- &#x2705; Pros: Easy to implement, widely supported\n- &#x274c; Cons: Less effective with extreme imbalance\n- &#x1f9e0; Good for: Logistic Regression, SVM, DL models\n- &#x1f3af; Suitable for: Classification, segmentation\n- &#x1f4c2; Data type: Any\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"112,118"}}],"payload":{"tag":"h3","lines":"110,111"}},{"content":"Cost-Sensitive Learning","children":[{"content":"<pre data-lines=\"121,127\"><code>- Design algorithm with misclassification costs\n- &#x2705; Pros: Flexible, works with any model\n- &#x274c; Cons: Requires cost matrix\n- &#x1f9e0; Best for: Business domains (fraud, health)\n- &#x1f3af; Suitable for: Classification, regression\n- &#x1f4c2; Data type: Any\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"121,127"}}],"payload":{"tag":"h3","lines":"119,120"}},{"content":"Ensemble Methods","children":[{"content":"Balanced Random Forest","children":[{"content":"<pre data-lines=\"132,138\"><code>- Uses balanced bootstrap samples\n- &#x2705; Pros: Improves generalization\n- &#x274c; Cons: Slower training\n- &#x1f9e0; Good for: Tabular, moderate imbalance\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"132,138"}}],"payload":{"tag":"h4","lines":"130,131"}},{"content":"EasyEnsemble","children":[{"content":"<pre data-lines=\"141,147\"><code>- Combines multiple undersampled models\n- &#x2705; Pros: Strong learners, avoids overfitting\n- &#x274c; Cons: High training cost\n- &#x1f9e0; Good for: Large imbalance ratio\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"141,147"}}],"payload":{"tag":"h4","lines":"139,140"}},{"content":"RUSBoost","children":[{"content":"<pre data-lines=\"150,156\"><code>- Undersampling + Boosting\n- &#x2705; Pros: Boosts imbalanced performance\n- &#x274c; Cons: Sensitive to noise\n- &#x1f9e0; Best for: Risk prediction\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"150,156"}}],"payload":{"tag":"h4","lines":"148,149"}},{"content":"Bagging + Resampling","children":[{"content":"<pre data-lines=\"159,165\"><code>- Train on resampled subsets\n- &#x2705; Pros: Simple, scalable\n- &#x274c; Cons: Less effective on small data\n- &#x1f9e0; Use for: Ensemble boosting\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Tabular\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"159,165"}}],"payload":{"tag":"h4","lines":"157,158"}}],"payload":{"tag":"h3","lines":"128,129"}}],"payload":{"tag":"h2","lines":"108,109"}},{"content":"3. Evaluation Metrics","children":[{"content":"<pre data-lines=\"168,172\"><code>- Precision, Recall, F1-Score &#x2192; better than accuracy\n- ROC-AUC &#x2192; threshold insight\n- PR AUC &#x2192; best for high imbalance\n- Cohen&#x2019;s Kappa, MCC &#x2192; balanced evaluation\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"168,172"}}],"payload":{"tag":"h2","lines":"166,167"}},{"content":"4. Advanced Approaches","children":[{"content":"Meta-Learning for Imbalance","children":[{"content":"<pre data-lines=\"177,181\"><code>- Learn to resample/reweight using meta-algorithms\n- &#x1f9e0; Complex but flexible\n- &#x1f3af; Suitable for: Classification\n- &#x1f4c2; Data type: Any\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"177,181"}}],"payload":{"tag":"h3","lines":"175,176"}},{"content":"GAN-based Oversampling","children":[{"content":"<pre data-lines=\"184,190\"><code>- Generate synthetic samples using GANs\n- &#x2705; Pros: High diversity\n- &#x274c; Cons: Difficult training, mode collapse\n- &#x1f9e0; Good for: Image, text, time series\n- &#x1f3af; Suitable for: Classification, generative\n- &#x1f4c2; Data type: Image, text, time series\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"184,190"}}],"payload":{"tag":"h3","lines":"182,183"}},{"content":"Curriculum Learning","children":[{"content":"<pre data-lines=\"193,199\"><code>- Train from easy to hard samples\n- &#x2705; Pros: Mimics human learning\n- &#x274c; Cons: Needs sample scoring\n- &#x1f9e0; Experimental stage\n- &#x1f3af; Suitable for: Classification, generative\n- &#x1f4c2; Data type: Any\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"193,199"}}],"payload":{"tag":"h3","lines":"191,192"}},{"content":"One-Class Classification","children":[{"content":"<pre data-lines=\"202,206\"><code>- Learn only minority class\n- &#x1f9e0; For anomaly detection, extreme imbalance\n- &#x1f3af; Suitable for: Anomaly detection\n- &#x1f4c2; Data type: Any\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"202,206"}}],"payload":{"tag":"h3","lines":"200,201"}}],"payload":{"tag":"h2","lines":"173,174"}},{"content":"5. Application Scenarios","children":[{"content":"<pre data-lines=\"209,214\"><code>- Fraud detection &#x2192; High imbalance, cost-sensitive\n- Medical diagnosis &#x2192; Rare conditions\n- Customer churn &#x2192; Leaving users\n- Rare event detection &#x2192; Machine failure\n- Spam filtering &#x2192; Binary + evolving data\n</code></pre>","children":[],"payload":{"tag":"pre","lines":"209,214"}}],"payload":{"tag":"h2","lines":"207,208"}}],"payload":{"tag":"h1","lines":"10,11"}},{"colorFreezeLevel":0,"initialExpandLevel":1,"activeNode":null,"placement":"center"})</script>
</body>
</html>
